# Generate synthetic face data using EG3D

<img src="https://github.com/cantonioupao/generate-synthetic-face-data/blob/main/assets/eg3d_training.gif" alt="drawing" width="600" height="300"/>

This repository, generates photorealistic face synthetic data, as well as the the corresponding depthmaps and 3D face structure/mesh using plug & play notebooks inspired by [EG3D](https://github.com/NVlabs/eg3d). In essence, EG3D is leveraged to generate synthetic face data and extract the respective face meshes, generated by EG3D or extracted from the corresponding depthmaps. The generated EG3D .obj meshes have higher facial detail, in contrast to the meshes extracted from the underlying 128x128 depthmaps.
Below you can find the relavant notebooks:
| Description      | Link |
| ----------- | ----------- |
| Generate EG3D face data (image, depthmap and 3D .obj)| [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cantonioupao/generate-synthetic-face-data/blob/main/colab_notebooks/eg3d.ipynb)|
| Visualize synthetic face mesh | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cantonioupao/generate-synthetic-face-data/blob/main/colab_notebooks/visualize_mesh.ipynb)|

The notebooks above, include detailed steps and instruction on generating synthetic EG3D data of fake photorealistic faces. Additionally, we project real images, to EG3D's latent space (PTI-inversion), to reproduce-generate custom real images, as high-quality EG3D faces (with the corresponding depthmap and 3D face structure). Finally the output mesh files (.obj, .ply, .mrc) of the corresponding generated face images, can be further visualized.

This repository is inspired by the EG3D [paper](https://nvlabs.github.io/eg3d/media/eg3d.pdf) and uses the codebase from [EG3D](https://github.com/NVlabs/eg3d). Please refer to the [project's webpage](https://arxiv.org/pdf/2112.07945.pdf) for more information.

![eg3d_training_dataset](https://github.com/cantonioupao/generate-synthetic-face-data/blob/main/assets/eg3d_synthetic_training.png)

By leveraging EG3D, the synthetic training set for training our depth & latent code regressor models was achieved. The training set consists of 50,000 samples of high-quality face images (.png) (512x512), the corresponding depthmaps(.png) (128x128), the 3D meshes (.obj), the EG3D latent code (.npy) and the corresponding camera parameters (.npy) (25,1) (16 extrinsic and 9 instrinsic parameters). The training dataset, to be generated requires 21 GPU days on a Tesla P100 GPU. The training dataset link can be downloaded using the link [here](https://drive.google.com/drive/folders/1-1T7kHcux_v9rjK6AJ_5uH9n3zuka09E?usp=sharing). The training dataset generated face images, from various viewpoints, to ensure the robustness of the trained models. The various viewpoint categories and the respective dataset percentages are depicted in the image above. 

The inversion method for EG3D is inspired and heavily borrows from the PTI-inversion method implementation from the following repository, found [here](https://github.com/oneThousand1000/EG3D-projector). Our own implementation of EG3D inversion, leveraged in the notebooks, can be found [here](https://github.com/cantonioupao/PTI-Inversion-EG3D)

